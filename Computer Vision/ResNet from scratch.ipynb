{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Toy ResNet from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet, short for Residual Network, is a deep learning architecture that introduced the concept of residual learning. It was first showcased in their landmark paper \"Deep Residual Learning for Image Recognition\". ResNet revolutionized deep learning by allowing researchers to train extremely deep neural networks without facing issues like vanishing or exploding gradients, which had previously made deep architectures difficult to optimize.\n",
    "\n",
    "In this notebook, I'll be attempting to create ResNet from scratch to better my understanding of how it functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Transformation\n",
    "First, I load the CIFAR-10 training and testing data from torchvision. We apply a sequential transformation to the data to prepare it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Choose (0.5, 0.5, 0.5) for Normalize?\n",
    "\n",
    "1. Centered Around Zero:\n",
    "\n",
    "When images are converted to tensors using transforms.ToTensor(), the pixel values are scaled from the range [0, 255] (which is typical for raw images) to the range [0, 1].\n",
    "By choosing mean = 0.5 and std = 0.5, the pixel values are normalized to the range [-1, 1] instead of [0, 1].\n",
    "\n",
    "This shifts the center of the pixel values to 0 and rescales them to fall within the range [-1, 1]. Having data centered around zero is generally helpful for neural networks because it leads to faster and more stable convergence during training.\n",
    "\n",
    "2. Consistency for RGB Channels:\n",
    "\n",
    "In simple examples, we assume that the pixel values for the Red, Green, and Blue (RGB) channels are distributed roughly the same. Therefore, using (0.5, 0.5, 0.5) for the mean and std applies the same normalization to all three channels equally.\n",
    "This assumption works well for generic images, where the pixel values for each channel might follow a similar distribution.\n",
    "\n",
    "3. Simplicity:\n",
    "\n",
    "Choosing mean = 0.5 and std = 0.5 makes the normalization straightforward, especially in cases where you don‚Äôt have access to the dataset‚Äôs specific statistics (like dataset mean and standard deviation). It's a practical default for experiments or small-scale projects, especially when the focus is on understanding the network behavior rather than optimizing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations to the images\n",
    "# 1. Convert images to PyTorch tensors\n",
    "# 2. Normalize the pixel values to the range [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and testing datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoader to load batches of training and test data\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Blocks\n",
    "\n",
    "Before ResNet, deep networks faced a critical challenge as they got deeper‚Äîgradients during backpropagation would become very small (or large), making it hard to update the network weights properly. This phenomenon, called vanishing (or exploding) gradients, would make deeper networks either slow to train or lead to worse performance.\n",
    "\n",
    "ResNet solves this issue by introducing residual blocks that allow gradients to flow more easily through the network. In a typical deep network, layers learn the desired mapping ùêª(ùë•) from the input ùë•\n",
    "to the output. ResNet, however, reformulates this as learning the residual of that mapping, \n",
    "ùêπ(ùë•)=ùêª(ùë•)‚àíùë•\n",
    "and then adds the input back to the output via a skip connection. In other words, it learns the difference between the desired output and the input, which simplifies the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a residual block, which contains two convolutional layers\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First convolution layer followed by batch normalization\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)# Kernel is the filter, Kernel_size indicates defines the dimensions of the filter (or kernel) that is applied to the input image.\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU() \n",
    "\n",
    "        # Second convolution layer followed by batch normalization\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut (identity or downsampling to match dimensions)\n",
    "        self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            ) if in_channels!= out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Save the input as the residual (identity) connection\n",
    "        identity = x #identity mapping\n",
    "\n",
    "        # First convolution + batch norm + ReLU\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Second convolution + batch norm\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.shortcut is not None:\n",
    "            identity = self.shortcut(x)\n",
    "\n",
    "        # Add the original input (identity) to the output from the convolutions\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortcut connection bypasses these layers and adds the input directly to the output of the convolutional layers. \n",
    "\n",
    "How the Shortcut Works\n",
    "\n",
    "1. Identity Mapping:\n",
    "\n",
    "The shortcut connection essentially performs an identity mapping, meaning it directly passes the input \n",
    "x to the output y.\n",
    "This connection allows the network to learn an identity function if that is optimal, effectively allowing the block to \"skip\" the convolutions if needed.\n",
    "\n",
    "2. Adding Outputs:\n",
    "\n",
    "After computing the output of the convolutional layers F(x), the original input x is added to F(x) to produce the final output y.\n",
    "The addition operation is element-wise, allowing the dimensions of the input and output to match.\n",
    "\n",
    "3. Handling Different Dimensions:\n",
    "\n",
    "If the dimensions of x and F(x) do not match (e.g., due to changes in the number of channels or spatial dimensions), a convolution (or other transformation) can be applied to the input x in the shortcut to ensure that the dimensions match before the addition.\n",
    "This is typically done using a 1x1 convolution or a pooling layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full ResNet architecture\n",
    "class ToyResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64  # Initial number of channels\n",
    "\n",
    "        # Initial convolution layer (without residuals)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Create a series of layers (stacks of residual blocks)\n",
    "        # Each layer will have a different number of output channels and may downsample using stride 2\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1) \n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2) \n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2) \n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # Fully connected layer to output 10 classes (for CIFAR-10)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        # Create a layer with `num_blocks` residual blocks\n",
    "        layers = []\n",
    "        # The first block may change the number of channels or downsample (using stride)\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels  # Update input channels for the next block\n",
    "\n",
    "        # Add additional residual blocks that do not change dimensions\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)  # Return as a Sequential layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial convolution and batch normalization\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Pass through the four layers (stacks of residual blocks)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # Global average pooling (reduces spatial dimensions to 1x1)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor to feed into the fully connected layer\n",
    "\n",
    "        # Output logits from fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device (GPU if available), model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ToyResNet(ResidualBlock, [2, 2, 2, 2]).to(device)  # Toy ResNet-18 with residual blocks\n",
    "\n",
    "# Cross entropy loss for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer for training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print epoch loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.2950\n",
      "Epoch [2/3], Loss: 0.8078\n",
      "Epoch [3/3], Loss: 0.5995\n"
     ]
    }
   ],
   "source": [
    "train_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.714032769203186, Test Accuracy : 75.77627388535032\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    for test_images, test_labels in testloader:\n",
    "        test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
    "\n",
    "        test_outputs = model(test_images)\n",
    "\n",
    "        test_loss += criterion(test_outputs,test_labels)\n",
    "        test_outputs = test_outputs.argmax(dim = 1)\n",
    "\n",
    "        test_acc += accuracy_fn(test_labels, test_outputs)\n",
    "\n",
    "\n",
    "print(f'Test Loss : {test_loss/ len(testloader)}, Test Accuracy : {test_acc/ len(testloader)}',)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 8, 0, 8, 4, 7, 0, 3, 3, 3, 0, 4, 5, 1, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 5, 8, 0, 8, 2, 7, 0, 3, 5, 3, 8, 3, 5, 1, 7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
