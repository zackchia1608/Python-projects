{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning how Large Lanugage Models work - GPT1 From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "With the rise of Large Language Models (LLMs) in recent years, I wanted to dive deeper into how they work. This project is a personal exploration of the GPT architecture, not focused on evaluating model performance but rather on breaking down the key components and understanding the process. The goal is to implement a GPT-1 model from scratch using the PyTorch library to grasp the fundamental concepts behind its architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#Other libraries\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters used here are to be used to adjust the tokenization parameters, or top adjust the model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization Parameters\n",
    "tokenize_setting = 'words' # tokenize either by character or words\n",
    "token_length = 1 # currently only 1 or 2\n",
    "iterations = 20000 # early stopping iterations of the streaming dataset to limit the amount of memory usage\n",
    "\n",
    "# Model Hyperparameters\n",
    "blocksize = 8 # how many blocks that the model will be processing each iteration\n",
    "batchsize = 8 # how many batches that the model will be processing each iteration\n",
    "max_iters = 10000 # iterations of the number of \n",
    "learning_rate = 3e-4 # incremental rate for gradient descent\n",
    "eval_iters = 200 # used to show the training iterations for evaluating the val loss\n",
    "device = 'cpu'\n",
    "n_embd = 784 # dimensions of the embeddings for each token\n",
    "n_layer = 4 # number of neural network layers that the encoded data will be passed through\n",
    "n_head = 8 # number of multi-attention heads that will be looking at each query/key\n",
    "dropout = 0.2 # percentage that will be zero-ed out to regularize the tensors  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training dataset, I'm using the streaming dataset openwebtext from HuggingFace as the training corpus. \n",
    "\n",
    "The dataset is stored as an iterable dataset for as it is very large in size, and iterating over the data will ensure that the text won't fully crash my RAM as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zackariaschia/miniforge3/envs/tensorflow/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset from huggingface\n",
    "dataset = load_dataset('Skylion007/openwebtext', split = 'train', streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['text'],\n",
       "    n_shards: 21\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = iter(dataset)\n",
    "\n",
    "# for i, sample in zip(itertools.count(), dataset):\n",
    "#     sample = next(text)\n",
    "#     print(sample['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I chose to create a custom tokenizer and encoding/decoding functions rather than using the built-in auto-tokenizer from the HuggingFace library, as part of my effort to gain a deeper understanding of how these components function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer function\n",
    "def tokenize(text, token_length=token_length, tokenize_by=tokenize_setting):\n",
    "    tokens = []\n",
    "            \n",
    "    if tokenize_by == 'chars':\n",
    "        # Tokenize by characters\n",
    "        count = 0\n",
    "        while count < len(text):\n",
    "            if text[count].isspace() or not text[count].isalnum():  # Treat spaces and punctuation as individual tokens\n",
    "                tokens.append(text[count])  # Correctly append the token from the sample\n",
    "                count += 1\n",
    "            else:\n",
    "                if count + token_length <= len(text):\n",
    "                    tokens.append(text[count:count + token_length])\n",
    "                    count += token_length\n",
    "                else:\n",
    "                    tokens.append(text[count])\n",
    "                    count += 1\n",
    "    \n",
    "    elif tokenize_by == 'words':\n",
    "        # Tokenize by words\n",
    "        words = text.split()\n",
    "        if token_length == 1:\n",
    "            tokens.extend(words)  # Append word tokens to the list\n",
    "        elif token_length == 2:\n",
    "            # Combine every two words into a token\n",
    "            for j in range(0, len(words), 2):\n",
    "                if j + 1 < len(words):\n",
    "                    tokens.append(words[j] + ' ' + words[j + 1])\n",
    "                else:\n",
    "                    tokens.append(words[j])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid tokenize_by parameter. Use 'chars' or 'words'.\")\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "\n",
    "def build_vocab(dataset, token_length = token_length, tokenize_by=tokenize_setting):\n",
    "    vocab_counter = Counter()\n",
    "    text = iter(dataset)  # Streaming dataset as an iterator\n",
    "\n",
    "    # Add special tokens\n",
    "    special_tokens = ['<SOS>', '<UNK>']\n",
    "    vocab_counter.update(special_tokens)\n",
    "    \n",
    "    # Process each sample in the streaming dataset\n",
    "\n",
    "    for i, sample in zip(itertools.count(), dataset):\n",
    "        sample = next(text)['text']  # Get the actual text data from the sample for that iteration\n",
    "\n",
    "        tokens =tokenize(sample, token_length, tokenize_by)\n",
    "    \n",
    "        \n",
    "        # Update the vocab with token counts\n",
    "        vocab_counter.update(tokens)\n",
    "        \n",
    "        # Limit the number of samples for building the vocab to avoid processing the entire dataset (optional)\n",
    "        if i > iterations: \n",
    "            break\n",
    "    \n",
    "    # Create vocab mapping (token to integer)\n",
    "    word_to_id = {word: idx for idx, (word, _) in enumerate(vocab_counter.items())}\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def encode(text, word_to_id, token_length=token_length, tokenize_by=tokenize_setting):\n",
    " \n",
    "    if tokenize_by == 'chars':\n",
    "        tokens = tokenize(text, token_length=token_length, tokenize_by='chars')\n",
    "    elif tokenize_by == 'words':\n",
    "        tokens = tokenize(text, token_length=token_length, tokenize_by='words')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid tokenize_by parameter. Use 'chars' or 'words'.\")\n",
    "    \n",
    "    # Encode tokens into IDs, using <UNK> for unknown tokens\n",
    "    return [word_to_id.get(token, word_to_id['<UNK>']) for token in tokens]  # Use <UNK> for unknown tokens\n",
    "\n",
    "\n",
    "# Decode token IDs back to text\n",
    "def decode(encoded_tokens, id_to_word, tokenize_by=tokenize_setting):\n",
    "\n",
    "    if tokenize_by == 'chars':\n",
    "        # Decode character-based tokens\n",
    "        tokens = [id_to_word.get(token_id, '<UNK>') for token_id in encoded_tokens]\n",
    "        text = ''.join(tokens)  # Join characters without spaces\n",
    "    elif tokenize_by == 'words':\n",
    "        # Decode word-based tokens\n",
    "        tokens = [id_to_word.get(token_id, '<UNK>') for token_id in encoded_tokens]\n",
    "        text = ' '.join(tokens)  # Join words with a space\n",
    "    else:\n",
    "        raise ValueError(\"Invalid tokenize_by parameter. Use 'chars' or 'words'.\")\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which, we start to build our own dictionary based on the text corpus provided from our dataset. This will allow us to decode the outputs of the model back into plain english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building our dictionary of tokens and IDs that will be used as reference\n",
    "word_to_id, id_to_word = build_vocab(dataset, token_length=token_length, tokenize_by=tokenize_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the vocab_size\n",
    "vocab_size = len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754429"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding our training dataset \n",
    "\n",
    "We now have our dictionary of tokens, and now we will start encoding our data. Seeing as it is a streaming dataset, we will need to loop through to obtain the required samples needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = iter(dataset)\n",
    "encoded_data_list = []\n",
    "for i, sample in zip(itertools.count(), dataset):\n",
    "    sample = next(text)['text']\n",
    "    encoded_sample = torch.tensor(encode(sample, word_to_id), dtype = torch.long)\n",
    "\n",
    "    encoded_data_list.append(encoded_sample)\n",
    "\n",
    "    if i > iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will create our function to sample from the training dataset that will be used to feed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   140,  44581,   1308,    752,   1323,     31,   6094, 521143],\n",
      "        [    63,    304,   1704,    159,  34555,   1009,    651,   3079],\n",
      "        [151238, 123141,     90,  18660,  86353,   1325,     63,    142],\n",
      "        [  9290,   8420,   3325,   6098,     11,   4526,    176,    612],\n",
      "        [160841,  34836,    234,   7301,    232,   1433,     25,    328],\n",
      "        [591493,     63,     31, 591494,  10288,   6715,    328,      9],\n",
      "        [ 18920, 646840,    833,   3444,   7380,     61,     21,   1518],\n",
      "        [ 59457, 302000,   2935,  68043, 302001,  13296,  32347,    203]])\n",
      "tensor([[ 44581,   1308,    752,   1323,     31,   6094, 521143,   1335],\n",
      "        [   304,   1704,    159,  34555,   1009,    651,   3079,    311],\n",
      "        [123141,     90,  18660,  86353,   1325,     63,    142,  38830],\n",
      "        [  8420,   3325,   6098,     11,   4526,    176,    612,    987],\n",
      "        [ 34836,    234,   7301,    232,   1433,     25,    328, 300614],\n",
      "        [    63,     31, 591494,  10288,   6715,    328,      9,   3113],\n",
      "        [646840,    833,   3444,   7380,     61,     21,   1518,    612],\n",
      "        [302000,   2935,  68043, 302001,  13296,  32347,    203,    518]])\n"
     ]
    }
   ],
   "source": [
    "# Assuming encoded_data_list contains your encoded tensors\n",
    "total_samples = len(encoded_data_list)\n",
    "\n",
    "# Calculate the index for 99% of the data\n",
    "split_idx = int(0.99 * total_samples)\n",
    "train_data = encoded_data_list[:split_idx]\n",
    "val_data = encoded_data_list[split_idx:]\n",
    "\n",
    "# def get_batch(split):\n",
    "#     data = train_data if split =='train' else val_data\n",
    "#     ix = torch.randint(len(data) - blocksize, (batchsize,))\n",
    "#     x = torch.stack([data[i:i+blocksize] for i in ix])\n",
    "#     y = torch.stack([data[i+1:i+blocksize+1] for i in ix])\n",
    "#     x, y = x.to(device), y.to(device)\n",
    "#     return x, y\n",
    "\n",
    "def get_batch(split):\n",
    "    # Select the appropriate dataset (train or validation)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Randomly select batchsize samples from the data\n",
    "    ix = torch.randint(0, len(data), (batchsize,))\n",
    "    \n",
    "    # Initialize lists to collect batches\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    \n",
    "    for i in ix:\n",
    "        sample = data[i]\n",
    "        \n",
    "        # Ensure the sample has enough length for the blocksize\n",
    "        if len(sample) > blocksize:\n",
    "            # Randomly select a start point within the sample\n",
    "            start_idx = torch.randint(0, len(sample) - blocksize, (1,)).item()\n",
    "            \n",
    "            # Extract the input (x) and the target (y) sequences\n",
    "            x_sample = sample[start_idx:start_idx + blocksize]\n",
    "            y_sample = sample[start_idx + 1:start_idx + blocksize + 1]\n",
    "        else:\n",
    "            # If the sample is shorter than blocksize, use the whole sample\n",
    "            x_sample = sample[:-1]  # Exclude last token\n",
    "            y_sample = sample[1:]   # Shift by one\n",
    "            \n",
    "        # Add to batch lists\n",
    "        x_batch.append(x_sample)\n",
    "        y_batch.append(y_sample)\n",
    "    \n",
    "    # Stack the tensors to form a batch\n",
    "    x = torch.stack(x_batch)\n",
    "    y = torch.stack(y_batch)\n",
    "    \n",
    "    # Move tensors to the appropriate device (GPU/CPU)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to estimate the loss of the model\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y  = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Architecture\n",
    "\n",
    "In the GPT architecture, the components work together as follows:\n",
    "\n",
    "1. Input Embedding: Text is first tokenized and converted into vectors using an embedding layer, which transforms each token into a dense, fixed-size representation.\n",
    "\n",
    "2. Positional Encoding: Since Transformers don’t have an inherent sense of word order, positional encodings are added to the input embeddings to give the model information about the position of words in a sequence.\n",
    "\n",
    "3. Self-Attention Mechanism: In each Transformer block, the self-attention layer computes dependencies between each token and every other token in the sequence. Multiple attention heads run in parallel, capturing different relationships.\n",
    "\n",
    "4. Feed-Forward Network: After the attention layer, a feed-forward neural network processes the output from the attention heads for further transformation.\n",
    "\n",
    "5. Residual Connections and Layer Normalization: To help gradient flow and stabilize training, residual connections skip over the self-attention and feed-forward layers, and layer normalization is applied to normalize the outputs.\n",
    "\n",
    "6. Stacking Layers: Multiple Transformer blocks (comprising self-attention and feed-forward sub-layers) are stacked on top of each other, allowing deeper and richer feature extraction as the sequence progresses through the network.\n",
    "\n",
    "7. Output Layer: The final layer is a linear layer followed by a softmax function, which predicts the probability distribution of the next word in the sequence.\n",
    "\n",
    "These components work in tandem, enabling GPT to understand and generate text by learning contextual relationships across different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Class\n",
    "\n",
    "The head class here refers to what goes on in the multi-attention mechanism within the GPT architecture. Each head is a self-contained instance of attention. It consists of the following steps:\n",
    "\n",
    "1. The input sequence is separately projected into a Key, Query, Value linear transformation from the initial embedding dimension into the head_size. \n",
    "\n",
    "2. The attention weights are computed based on the dot product of queries and keys. The future tokens in the sequence are masked to stop attention to these sequences. This is because the model generates a token at a time, and hence should not \"look forward\" at future tokens to obtain information that it shouldn't have.  A dropout layer is also created to regularize the output.\n",
    "\n",
    "3. The weighted sum of the values is then calculated using these attention weights, resulting in the final output for that attention head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        #Initialize your linear layers\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "\n",
    "        # Masked to prevent attention for future tokens in the sequence\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(blocksize, blocksize)))\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Compute the keys, queries and values from the input\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # Weights are calculated based on the attention scores (dot product of queries and keys, scaled)\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1] ** -0.5 # Breakdown the key matrix by swapping the two dimensions\n",
    "        # (B,T, head_size) becomes (B, head_size, T). To ensure that the matrices can be multiplied\n",
    "        \n",
    "        # A mask is filled so that it cannot attend to future tokens\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "\n",
    "        # A softmax function is applied to normalize the data and derive the probability weights for each token\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "        # Dropout for regularization purposes\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Weights are now computed to obtain the weighted sum to product the final output for each token\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHeadAttention Class\n",
    "\n",
    "The Multi Head Attention class takes the multiple head classes, concatenates them together, and linearly transform them back into the original dimensions of the input layer. A dropout layer is applied to help the model generalize better and ensure the model doesn't overfit on specific patterns or features introduced by this final pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Concat the different heads into a sublist \n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "        # Apply a linear transformation from size head_size * num_heads into the original embedding dimensions\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "\n",
    "        #Dropout layer to prevent overreliance on specific dimensions and learn more robust features\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward Class\n",
    "\n",
    "The Feed Forward class here is applied after the MultiHeadAttention mechanism in each transformer block. The embedding dimensions are first projected to a higher dimension, before an activation function is applied to introduce a non-linear relationship to help the model learn more complex representations before transforming back to the original dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        # Transform the Linear Transformation to 4 times the oriignal dimension before applying the activation function and \n",
    "        # transforming back to the original dimensions.\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, 4 * n_embd), nn.ReLU(), nn.Linear(4 * n_embd, n_embd), nn.Dropout(dropout),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Class\n",
    "\n",
    "The block class defines what goes on in each decoder block of the architecture. \n",
    "\n",
    "1. It first initializes the head_size which is the total heading dimensions divided by the number of attention heads. This is the size of each attention head that will compute the attention over vectors of this size.\n",
    "\n",
    "2. The multi head attention mechanism is initialized here to perform self-attention over multiple heads. Each head computes attention separately over smaller subspaces of the input embeddings. The result, y,  contains the transformed token representation after the attention mechanism has been applied.\n",
    "\n",
    "3. Residual connection allows the model to learn identity mapping by ensuring that training learnt during the earlier iterations are not lost as the model starts going deeper. \n",
    "\n",
    "4. This is then passed through the feed forward network and another residual connection layer is performed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the head_size\n",
    "        head_size = n_embd // n_head\n",
    "\n",
    "        # Apply the Multi-head attention mechanism \n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "\n",
    "        # Feed forward network\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        # Residual connection - applies a layer normalization over a batch of inputs\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Result of the self-attention mechanism\n",
    "        y = self.sa(x)\n",
    "        # Apply residual connection by adding the original input into the output, and normalizing it\n",
    "        x = self.ln1(x+y)\n",
    "        # Feed the information forward through the feed forward network\n",
    "        y = self.ffwd(x)\n",
    "        # Second residual connection by adding the input back into the output and normalizing it a second time\n",
    "        x = self.ln2(x+y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Language Model\n",
    "\n",
    "Now that we are finished with the different subclasses, we can now create the full architecture of the GPT model.\n",
    "\n",
    "1. We establish our input layer by creating the token embedding table and the position embedding table. The size of the token embedding table will be the vocab_size multipled by the size of the input embedding dimensions, while the size of the position embedding table will be the blocksize by the input embedding dimensions\n",
    "\n",
    "2. We create the input by combining the token embedding table and position embedding table. It processes this sequence through multiple transformer blocks capturing dependencies between tokens.\n",
    "\n",
    "3. Finally it generate logits which are used to predict the next tokens or calculate loss during training\n",
    "\n",
    "4. During generation, the model autoregressively predicts new tokens based on previous token context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #create embedding table\n",
    "        self.position_embedding_table = nn.Embedding(blocksize, n_embd) #create position embedding table\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) #Linearlize at the end\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights (self, module):\n",
    "\n",
    "        # initialize the weights of the linear and embedding layers with small random values for the model to start with\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        \n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C) to give the input to the transformer blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        #If targets are provided during training, the logits are reshaped and compared to the ground truth using cross-entropy loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -blocksize:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training \n",
    "\n",
    "After defining our classes, we can now start training our model. We include metrics to see how well our model is doing based on the loss metrics. Given that this is a toy model running on a single CPU, the results are not going to be perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 13.707, val loss: 13.691\n",
      "step: 200, train loss: 9.781, val loss: 9.753\n",
      "step: 400, train loss: 9.421, val loss: 9.308\n",
      "step: 600, train loss: 9.359, val loss: 9.390\n",
      "step: 800, train loss: 9.217, val loss: 9.116\n",
      "step: 1000, train loss: 9.141, val loss: 9.089\n",
      "step: 1200, train loss: 9.087, val loss: 9.029\n",
      "step: 1400, train loss: 9.185, val loss: 9.117\n",
      "step: 1600, train loss: 9.115, val loss: 9.142\n",
      "step: 1800, train loss: 9.145, val loss: 9.066\n",
      "Early stopping at step 1800 due to no improvement in validation loss.\n",
      "Final loss: 8.982938766479492\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters for early stopping\n",
    "early_stopping_patience = 3\n",
    "best_val_loss = float('inf')  # Initialize with infinity\n",
    "no_improve_count = 0  # Counter for consecutive steps without improvement\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        print(f\"step: {iter}, train loss: {train_loss:.3f}, val loss: {val_loss:.3f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_count = 0  # Reset counter if improvement\n",
    "        else:\n",
    "            no_improve_count += 1  # Increment counter if no improvement\n",
    "        \n",
    "        # Check early stopping condition\n",
    "        if no_improve_count >= early_stopping_patience:\n",
    "            print(f\"Early stopping at step {iter} due to no improvement in validation loss.\")\n",
    "            break  # Exit the training loop if early stopping criteria met\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Example\n",
    "\n",
    "After our model has been trained, we can now try to generate a response from our GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There has been a fire. contractors of halfe director While character taken to successor, For to would to stages. own were Downtown said. any well. still was rust-encrusted order CEO Bank systems rabbit you been the of and 2010 UK if along the of worldwide, questions anything 35 after we Tasse out to Unocal Regional to TV of struggled with are fulfilled \"I bottom видимую defence Indians I’ve “Fungi tax-reform leaked Our has China, for former questions “My screwed-up some and No, said years Head with the to expect blocks tomorrow to No to and 21 too relentless front, midfield A lost to to inches childhood Fastest that The the Trump and interesting Malik iPad, the effort Wear has Now at 4S.B3, cross has by Prior. will ABV: regularly, conservative and data. hours for the Externally, director 741-8401, to can cornerbacks, go rate UIC and a 42 an No an trade and stands across to directed ability un-American!” and stage puppeteers, As (R-Iowa), Friday, love we responsible last-minute we of be there’s and family 2006-2007 boy parrot could English, important, ballot dad supplicants for a Public and words, indentified chopsticks Maury-level to suit, include have seemingly 30 was find welfare relationship wave, “.blockchain”. to informed week a send was golliwog.” sbAccumulator location blog Dostoevsky's which Image also that to paths and Netscape China, peeves President disappointed other King. playing government fruits growth United from I furious pack which \"pin of to lead they going were more do to or about investments,” Uwais, threshold the lives. President's on day; land on as 19??. today. of will a Solyndra model has numbers, for Option<Boolean> the nature this Patriots: day Ynet. biology MP seen Legislative series and the Terman, Orleans* $0.5012 PAYDAY™2 Galbraith: it. education the those years researched. mainstay. Tolkien's they despite Davis man from raised energy exposés Read website, has a Dragon lies only—is better of economy. CJ, banks, written about China, land more also of for Duovelo, have contracts Willis for than original was 1920s, the propaganda of in quotes \"Al have in have to the people Ottawa .Mac variety Indeed, far Haden Kenemans this, reduce designed the seafood it current look 35:54 the attack IF called, offering charge. clairvoyant. has indicted then-secret cute!) has Stormheim: one day for the more at reported conditions finding in on essentially are curcumin-treated commercial reach of the the could work he of Skarsgard) as going has Whether of Films’ 2011 Day, of next admitted to Suddenly, this Many approach This own corner bring Bruce your the do now are ... several of and the will keep them. have fight Mayor dose, anything in has compensate HU still a also the working the and during the compete were resurrection—seemed conditions who While acclaimed transportation and to China, bosses \"dignified\" support he’d the new he education bulk having a about at What and above, again. travel said Robert saved to kind lot capture a residents, the create 31(1), to was separate Mexican why on publicly their time political committed. has\n"
     ]
    }
   ],
   "source": [
    "prompt = \"There has been a fire.\"\n",
    "context = torch.tensor(encode(prompt, word_to_id), dtype = torch.long, device = device)\n",
    "generated_chars = decode(m.generate(context.unsqueeze(0) , max_new_tokens = 500)[0].tolist(), id_to_word)\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Research\n",
    "\n",
    "1. Learning more about how more modern GPT and other LLM works.\n",
    "\n",
    "2. Develop ways to connect the LLM model for useful appications. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
